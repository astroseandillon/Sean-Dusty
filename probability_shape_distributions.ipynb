{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb7cf89",
   "metadata": {},
   "source": [
    "# Calculating the absorption and scattering cross section using a probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "935d1248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# AA = time.time()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.integrate as spit\n",
    "import os\n",
    "print('hello world')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ff311",
   "metadata": {},
   "source": [
    "### This defines the probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f45fc3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "def probability(dis_name, l1, l2, lmin=0.05, m1=0, m2=0, d=0):\n",
    "    '''\n",
    "    This is the probability distribution as a function of L1 and L2, the \n",
    "    geometric parameters. This parameter gets inserted into the integral that \n",
    "    calculates the average polarizability per unit volume\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dis_name : String\n",
    "        This specifies the distribution we will be using. \n",
    "        'CDE' = Continuous Distribution of Ellipsoids\n",
    "        'ERCDE' = Externally Restricted CDE, returns CDE if lmin=0\n",
    "        'tCDE' = truncated CDE, REQUIRES MORE WORK\n",
    "    l1 : Float\n",
    "        Largest Geometric Constant, lmin<l1<1.0\n",
    "    l2 : Float\n",
    "        Second Largest Geometric Constant, lmin<l2<=l1\n",
    "    lmin : Float, optional\n",
    "        Minimum allowed geometric constant.  The default is 0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Float\n",
    "        Function dependent on l1 and l2\n",
    "\n",
    "    '''\n",
    "    l3 = 1 - l1 - l2\n",
    "    if dis_name == 'CDE':\n",
    "        return 2\n",
    "    elif dis_name == 'CDE2':\n",
    "        return 120 * l1 * l2 * l3\n",
    "    elif dis_name == 'ERCDE':\n",
    "        return 2/((1 - (3*lmin))**2)\n",
    "    elif dis_name == 'tCDE':\n",
    "        return 1/((1-d-m2)*(1-m1-m2-d) - 0.5*((1-d-m2)**2) - m1**2)\n",
    "    else:\n",
    "        return True\n",
    "        \n",
    "    \n",
    "print('hello world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe9a8fb",
   "metadata": {},
   "source": [
    "### I think it would be easier to make the tCDE its own function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74997615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_tCDE(m1,m2,d):\n",
    "    return 1/((1-d-m2)*(1-m1-m2-d) - 0.5*((1-d-m2)**2) - m1**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc5b2f5",
   "metadata": {},
   "source": [
    "### Now we define our volume function. v_avg is used to calculate sigma below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47c3f86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def volume_integrand_mrn(r, q):\n",
    "    v = r**(-q)\n",
    "    return v\n",
    "\n",
    "# UNITS ARE IN MICRONS\n",
    "rmin = 0.005\n",
    "rmax = 0.25\n",
    "q = 3.5\n",
    "\n",
    "r_integral = spit.quad(volume_integrand_mrn, rmin, rmax, args=q)\n",
    "r_average = ((1/(rmax - rmin)) * r_integral[0])**(1/-q)\n",
    "v_avg = (4./3.) * np.pi * r_average**3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba15ca05",
   "metadata": {},
   "source": [
    "### creates function that calculates Sigma, defined in eq 13 in Min et al 2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2fc664a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "def sigma(m, lamda, v):\n",
    "    sig = []\n",
    "    for i in range(len(lamda)):\n",
    "        k = (2.0 * np.pi)/lamda[i]\n",
    "        term1 = (6.0*np.pi) / (v * (k**3))\n",
    "        term2 = np.imag((m[i]**2))\n",
    "        term3 = 1.0 / abs(m[i]**2 - 1)**2\n",
    "        sig.append(term1 * term2 * term3)\n",
    "    return sig\n",
    "print('hello world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3c1396",
   "metadata": {},
   "source": [
    "### creates our bounds for our geometric factors. The bounds are the sides of a triangle in (l1, l2) space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b28b2101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "def bounds_l1():\n",
    "    return [0,1]\n",
    "\n",
    "def bounds_l2(l1):\n",
    "    return [0,1-l1]\n",
    "print('hello world')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d2ac26",
   "metadata": {},
   "source": [
    "### This is where we calculate the absorption cross-section (Cabs). It creates an empty list, then calculates Cabs for a given distribution at each wavelength as described in Min 03, eqn 15. It then uses this to find the shape averaged mass absorption coefficient for particles of a given volume, as described in Min 03, eqn 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36d4d2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cabs(m, dis_name, bounds_l2, bounds_l1):\n",
    "    cabs = []\n",
    "    if dis_name=='spheres':\n",
    "        for j in range(len(m)):\n",
    "            cabs.append(np.imag(3*(m[j]**2 - 1)/(m[j]**2 + 2)))\n",
    "    else:\n",
    "        for j in range(len(m)):\n",
    "            def f(l1, l2, n=m[j], dis_name='CDE'):\n",
    "                b = 1/(n**2 - 1)\n",
    "                term1 = 1/3 * 1/(b + l1)\n",
    "                term2 = 1/3 * 1/(b + l2)\n",
    "                term3 = 1/3 * 1/(b + 1 - l1 - l2)\n",
    "            # r = np.real((term1 + term2 + term3)*probability(dis_name, l1, l2))\n",
    "                j = np.imag((term1 + term2 + term3)*probability(dis_name, l1, l2))\n",
    "                return j\n",
    "            # return np.real((term1 + term2 + term3)*probability(dis_name, l1, l2)) + np.imag((term1 + term2 + term3)*probability(dis_name, l1, l2))\n",
    "            cabs.append(spit.nquad(f, [bounds_l2, bounds_l1])[0])\n",
    "    return cabs\n",
    "\n",
    "# j = cabs(m, 'spheres', bounds_l2, bounds_l1)\n",
    "# # dust = 'grph1-dl.nk'                  #DUST NAME HERE #grf\n",
    "# # rho = 3.33 #grams cm**-3            #density\n",
    "# # pathy = os.path.join(nk_path, dust) #pipeline is open\n",
    "# # wavelen, n_dust, k_dust = np.loadtxt(pathy, skiprows=7, unpack=True)\n",
    "# #                                     #lamda, n, and k values are extracted\n",
    "# # m = np.array([complex(n_dust[i], k_dust[i]) for i in range(len(wavelen))])\n",
    "# k = cabs(m, 'CDE', bounds_l2, bounds_l1)\n",
    "\n",
    "# print(j)\n",
    "# # print('')\n",
    "# print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac35618d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Specify which dust we are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "976b04e2-21b2-4875-a4af-c36ab384cbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dustlist = [('oliv_nk_x.nk', 'spheres'), ('oliv_nk_y.nk', 'spheres'), ('oliv_nk_z.nk', 'spheres')]\n",
    "namelist = [dustlist[j][0][:-3]+dustlist[j][1]+'.dat' for j in range(len(dustlist))]\n",
    "weightlist = [1.0, 1.0, 1.0]\n",
    "dust_dir = ['/home/physics/Research/DUSTY/DUSTY/Lib_nk/', \n",
    "            \"C:/UTSA/Research/DUSTY/DUSTY/Lib_nk/\",\n",
    "           \"C:/Users/uhe082/OneDrive - University of Texas at San Antonio/Lib_nk\"]\n",
    "# this is the possible locations of where dust can be\n",
    "\n",
    "\n",
    "nk_path = dust_dir[2]               #where the dust is \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46361663-ea83-4f41-a693-83366f7e20a8",
   "metadata": {},
   "source": [
    "# Calculate the absorption and scattering arrays for each dust at each dust shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0792f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(dustlist)):\n",
    "    pathy = os.path.join(nk_path, dustlist[j][0]) #pipeline is open\n",
    "    wavelen, n_dust, k_dust = np.loadtxt(pathy, skiprows=7, unpack=True)\n",
    "    m = np.array([complex(n_dust[i], k_dust[i]) for i in range(len(wavelen))])\n",
    "    cab = cabs(m, dustlist[j][1], bounds_l2, bounds_l1)\n",
    "    Cabs_array = np.array((cab))\n",
    "    Cabs_array *= (2 * np.pi / (wavelen)) * v_avg\n",
    "    sig = np.array((sigma(m, wavelen, v_avg)))\n",
    "    Csca_array = Cabs_array/sig\n",
    "    output = np.transpose((wavelen, Cabs_array, Csca_array))\n",
    "    f = open(dustlist[j][0][:-3]+dustlist[j][1]+'.dat', 'w')\n",
    "    for i in range(len(output)):\n",
    "        f.write(f\"{output[i,0]} \\t {output[i,1]} \\t {output[i,2]}\\n\")\n",
    "    f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b876fae6-8abb-428f-ab52-1137396bc351",
   "metadata": {},
   "source": [
    "# Making everything into the same array size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c54e7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "lam_final = np.geomspace(0.2, 500, num=500)\n",
    "total_array = np.ndarray((3,len(lam_final),len(dustlist)))\n",
    "total_array[:,:,0] = lam_final\n",
    "\n",
    "for k in range(len(namelist)):\n",
    "    lam, cabs_tot, csca_tot = np.loadtxt(namelist[k], unpack=True)\n",
    "    total_array[k,:,1] = np.interp(lam_final, lam, cabs_tot)\n",
    "    total_array[k,:,2] = np.interp(lam_final, lam, csca_tot)\n",
    "    h = open('tot array {}.txt'.format(k), 'w')\n",
    "    for b in range(len(lam_final)):\n",
    "        h.write(f\"{total_array[k,b,0]} \\t {total_array[k,b,1]} \\t {total_array[k,b,2]} \\n \")\n",
    "\n",
    "h.close()\n",
    "\n",
    "avg_array = np.ndarray((len(lam_final),3))\n",
    "avg_array[:,0] = lam_final\n",
    "for j in range(len(lam_final)):\n",
    "    avg_array[j,1] = np.average(total_array[:,j,1], weights=weightlist)\n",
    "    avg_array[j,2] = np.average(total_array[:,j,2], weights=weightlist)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "k = open('avg array.txt', 'w')\n",
    "for i in range(len(avg_array)):\n",
    "    k.write(f\"{avg_array[i,0]} \\t {avg_array[i,1]} \\t {avg_array[i,2]}\\n\")\n",
    "k.close()   \n",
    "    \n",
    "    \n",
    "    \n",
    "print('hello world')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5587b196-38f7-485f-a72d-a158c0916e0d",
   "metadata": {},
   "source": [
    "# Outputting into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10ffc7e4-fbbf-4bd5-90f2-4e288cead344",
   "metadata": {},
   "outputs": [],
   "source": [
    "titlestring=''\n",
    "for g in range(len(namelist)):\n",
    "    titlestring += namelist[g][:3] + str(weightlist[g]).replace('.','')\n",
    "    \n",
    "f = open(titlestring+'.dat','w')\n",
    "for i in range(len(lam_final)):\n",
    "    f.write(f\"{avg_array[i,0]} \\t {avg_array[i,1]} \\t {avg_array[i,2]}\\n\")\n",
    "f.close() \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4dc35e-3f79-4e28-b649-e6f1b18055ed",
   "metadata": {},
   "source": [
    "# Seeing what we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "94efd386",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\l'\n",
      "C:\\Users\\uhe082\\AppData\\Local\\Temp\\ipykernel_25580\\4232736442.py:6: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  '''\n",
      "C:\\Users\\uhe082\\AppData\\Local\\Temp\\ipykernel_25580\\4232736442.py:6: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  '''\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string 'x,' to float64 at row 0, column 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'x,'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# lam_new, cab_new, csa_new = \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# x,y = np.loadtxt('cde1_fab01_fig7_olivine.csv', delimiter='\\l',unpack=True, skiprows=1, usecols=[0,1])\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# lam_old, cab_old, csa_old = np.loadtxt('oli10oli10oli10.dat', unpack=True)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m data\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mloadtxt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcde1_fab01_fig7_olivine.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,usecols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03mfig, ax = plt.subplots()\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03mtitle = 'Fab test '\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03max.legend()\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:1373\u001b[0m, in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1371\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1373\u001b[0m arr \u001b[38;5;241m=\u001b[39m _read(fname, dtype\u001b[38;5;241m=\u001b[39mdtype, comment\u001b[38;5;241m=\u001b[39mcomment, delimiter\u001b[38;5;241m=\u001b[39mdelimiter,\n\u001b[0;32m   1374\u001b[0m             converters\u001b[38;5;241m=\u001b[39mconverters, skiplines\u001b[38;5;241m=\u001b[39mskiprows, usecols\u001b[38;5;241m=\u001b[39musecols,\n\u001b[0;32m   1375\u001b[0m             unpack\u001b[38;5;241m=\u001b[39munpack, ndmin\u001b[38;5;241m=\u001b[39mndmin, encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   1376\u001b[0m             max_rows\u001b[38;5;241m=\u001b[39mmax_rows, quote\u001b[38;5;241m=\u001b[39mquotechar)\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:1016\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     data \u001b[38;5;241m=\u001b[39m _preprocess_comments(data, comments, encoding)\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_dtype_via_object_chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1016\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _load_from_filelike(\n\u001b[0;32m   1017\u001b[0m         data, delimiter\u001b[38;5;241m=\u001b[39mdelimiter, comment\u001b[38;5;241m=\u001b[39mcomment, quote\u001b[38;5;241m=\u001b[39mquote,\n\u001b[0;32m   1018\u001b[0m         imaginary_unit\u001b[38;5;241m=\u001b[39mimaginary_unit,\n\u001b[0;32m   1019\u001b[0m         usecols\u001b[38;5;241m=\u001b[39musecols, skiplines\u001b[38;5;241m=\u001b[39mskiplines, max_rows\u001b[38;5;241m=\u001b[39mmax_rows,\n\u001b[0;32m   1020\u001b[0m         converters\u001b[38;5;241m=\u001b[39mconverters, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1021\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding, filelike\u001b[38;5;241m=\u001b[39mfilelike,\n\u001b[0;32m   1022\u001b[0m         byte_converters\u001b[38;5;241m=\u001b[39mbyte_converters)\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1025\u001b[0m     \u001b[38;5;66;03m# This branch reads the file into chunks of object arrays and then\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m     \u001b[38;5;66;03m# casts them to the desired actual dtype.  This ensures correct\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m     \u001b[38;5;66;03m# string-length and datetime-unit discovery (like `arr.astype()`).\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m     \u001b[38;5;66;03m# Due to chunking, certain error reports are less clear, currently.\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filelike:\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string 'x,' to float64 at row 0, column 1."
     ]
    }
   ],
   "source": [
    "# lam_new, cab_new, csa_new = \n",
    "# x,y = np.loadtxt('cde1_fab01_fig7_olivine.csv', delimiter='\\l',unpack=True, skiprows=1, usecols=[0,1])\n",
    "# lam_old, cab_old, csa_old = np.loadtxt('oli10oli10oli10.dat', unpack=True)\n",
    "data=np.loadtxt('cde1_fab01_fig7_olivine.csv',usecols=[0,1,2])\n",
    "\n",
    "'''\n",
    "fig, ax = plt.subplots()\n",
    "title = 'Fab test '\n",
    "ax.set(xscale='log', yscale='log')\n",
    "ax.set_title(title, fontsize=16)\n",
    "ax.set_xlabel(r'$\\lambda (\\mu m)$', fontsize=14)\n",
    "ax.set_ylabel(r'$<C_{abs}>$ cm$^{2}$ g$^{-1}$', fontsize=14)\n",
    "# ax.plot(lam_old, cab_old, label='our data')\n",
    "ax.plot(x,y, label='Fab fig 7')\n",
    "# ax.plot(wavelen, np.array((cabs_ercde)), label='ERCDE')\n",
    "ax.legend()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7fbaad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f441ab33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
